{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/yari/anaconda3/lib/python3.10/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/yari/anaconda3/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2a5109-1ed2-457c-82af-bb0cb9e79f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cb9dc0-2283-4322-a8e5-10fb164f27af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Comparative Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857ad32a-49e8-435d-a721-46560f45b1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_spark(file_path):\n",
    "    start_time = time.time()\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    duration = time.time() - start_time\n",
    "    return df, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28d1a7a-f22d-41d2-a959-e119a445d730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_spark_rename_columns(df):\n",
    "    start_time = time.time()\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name, f\"{col_name}_new\")\n",
    "    duration = time.time() - start_time\n",
    "    return duration\n",
    "    \n",
    "def transform_spark_drop_na(df):\n",
    "    start_time = time.time()\n",
    "    df = df.dropna()\n",
    "    duration = time.time() - start_time\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cf0716-9e71-4fba-80b5-5b5dfbb78e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_file_size(path):\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "    path = spark._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    \n",
    "    if not fs.exists(path):\n",
    "        raise FileNotFoundError(f\"The folder path does not exist: {path}\")\n",
    "\n",
    "    def calculate_size(folder):\n",
    "        file_status = fs.listStatus(folder)\n",
    "        total_size = 0\n",
    "        for status in file_status:\n",
    "            if status.isFile():\n",
    "                total_size += status.getLen()  \n",
    "            elif status.isDirectory():\n",
    "                total_size += calculate_size(status.getPath()) \n",
    "        return total_size\n",
    "\n",
    "    total_size_bytes = calculate_size(path)\n",
    "\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    return round(total_size_mb, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70783297-6372-4c01-be9f-c882906d511e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_spark_csv(df, output_path):\n",
    "    start_time = time.time()\n",
    "    df.write.csv(output_path, header=True, mode='overwrite')\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    file_size = get_file_size(output_path)\n",
    "\n",
    "    return duration, file_size\n",
    "\n",
    "\n",
    "def load_spark_parquet(df, output_path):\n",
    "    start_time = time.time()\n",
    "    df.write.parquet(output_path, mode='overwrite')\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    file_size = get_file_size(output_path)\n",
    "\n",
    "    return duration, file_size\n",
    "\n",
    "\n",
    "def load_spark_orc(df, output_path):\n",
    "\n",
    "    start_time = time.time()\n",
    "    df.write.format(\"orc\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    file_size = get_file_size(output_path)\n",
    "\n",
    "    return duration, file_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad5621e-da03-4f8b-9364-8036d80bc98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/10 18:22:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/12/10 18:22:48 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "/Users/yari/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process finished successfully!\n"
     ]
    }
   ],
   "source": [
    "datasets = ['transactions_data.csv', 'titanic.csv', 'reviews.csv', 'locations.csv']\n",
    "\n",
    "dim_datasets = []\n",
    "fact_metrics = []\n",
    "\n",
    "for index, dataset in enumerate(datasets):\n",
    "    primary_key = index + 1\n",
    "    path = dataset.split('.')[0]\n",
    "    \n",
    "    df_raw, extract_time = extract_spark(f\"data/{dataset}\")\n",
    "    \n",
    "    transform_rename_columns = transform_spark_rename_columns(df_raw)\n",
    "    transform_dropna = transform_spark_drop_na(df_raw)\n",
    "    \n",
    "    load_time_csv, file_size_csv = load_spark_csv(df_raw, f\"pyspark_analysis/{path}.csv\")\n",
    "    load_time_parquet, file_size_parquet = load_spark_parquet(df_raw, f\"pyspark_analysis/{path}.parquet\")\n",
    "    load_time_orc, file_size_orc = load_spark_orc(df_raw, f\"pyspark_analysis/{path}.orc\")\n",
    "    \n",
    "    dim_datasets.append({\n",
    "        \"id\": primary_key,\n",
    "        \"dataset_name\": path,\n",
    "        \"number_of_rows\": df_raw.count()\n",
    "    })\n",
    "    \n",
    "    fact_metrics.append({\n",
    "        \"dataset_id\": primary_key,\n",
    "        \"extract_time\": round(extract_time,2),\n",
    "        \"transform_rename_columns_time\": round(transform_rename_columns,2),\n",
    "        \"transform_dropna_time\": round(transform_dropna,2),\n",
    "        \"load_time_csv\": round(load_time_csv,2),\n",
    "        \"file_size_csv_mb\": round(file_size_csv / (1024 * 1024),2),\n",
    "        \"load_time_parquet\": round(load_time_parquet,2),\n",
    "        \"file_size_parquet_mb\": round(file_size_parquet / (1024 * 1024),2),\n",
    "        \"load_time_orc\": round(load_time_orc,2),\n",
    "        \"file_size_orc_mb\": round(file_size_orc / (1024 * 1024),2)\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "dim_datasets_df = spark.createDataFrame(dim_datasets)\n",
    "fact_metrics_df = spark.createDataFrame(fact_metrics)\n",
    "\n",
    "dim_datasets_pd = dim_datasets_df.toPandas()\n",
    "fact_metrics_pd = fact_metrics_df.toPandas()\n",
    "\n",
    "dim_datasets_pd.to_csv(\"pyspark_analysis/dim_datasets.csv\", index=False)\n",
    "fact_metrics_pd.to_csv(\"pyspark_analysis/fact_metrics_pyspark.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Process finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>id</th>\n",
       "      <th>number_of_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transactions_data</td>\n",
       "      <td>1</td>\n",
       "      <td>13305915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>titanic</td>\n",
       "      <td>2</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviews</td>\n",
       "      <td>3</td>\n",
       "      <td>703796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>locations</td>\n",
       "      <td>4</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset_name  id  number_of_rows\n",
       "0  transactions_data   1        13305915\n",
       "1            titanic   2             891\n",
       "2            reviews   3          703796\n",
       "3          locations   4             845"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim = pd.read_csv(\"pyspark_analysis/dim_datasets.csv\", sep=',')\n",
    "df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>extract_time</th>\n",
       "      <th>file_size_csv_mb</th>\n",
       "      <th>file_size_orc_mb</th>\n",
       "      <th>file_size_parquet_mb</th>\n",
       "      <th>load_time_csv</th>\n",
       "      <th>load_time_orc</th>\n",
       "      <th>load_time_parquet</th>\n",
       "      <th>transform_dropna_time</th>\n",
       "      <th>transform_rename_columns_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.74</td>\n",
       "      <td>26.78</td>\n",
       "      <td>23.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset_id  extract_time  file_size_csv_mb  file_size_orc_mb  \\\n",
       "0           1         11.46               0.0               0.0   \n",
       "1           2          0.31               0.0               0.0   \n",
       "2           3          1.11               0.0               0.0   \n",
       "3           4          0.21               0.0               0.0   \n",
       "\n",
       "   file_size_parquet_mb  load_time_csv  load_time_orc  load_time_parquet  \\\n",
       "0                   0.0          22.74          26.78              23.08   \n",
       "1                   0.0           0.22           0.22               0.24   \n",
       "2                   0.0           0.97           1.18               0.72   \n",
       "3                   0.0           0.20           0.17               0.18   \n",
       "\n",
       "   transform_dropna_time  transform_rename_columns_time  \n",
       "0                   0.01                           0.08  \n",
       "1                   0.02                           0.09  \n",
       "2                   0.01                           0.01  \n",
       "3                   0.01                           0.10  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact = pd.read_csv(\"pyspark_analysis/fact_metrics_pyspark.csv\", sep=',')\n",
    "df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark_analysis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
